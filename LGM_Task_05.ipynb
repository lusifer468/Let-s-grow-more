{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfp7vImOfHdNgYdT7Uubih"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fQ7WJTjvAslA"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import pickle\n","import numpy as np\n","\n","nltk.download('punkt')\n","\n","# Example dataset\n","data = [\n","    {\"intent\": \"greeting\", \"patterns\": [\"Hi\", \"Hello\", \"Hey\"], \"responses\": [\"Hello!\", \"Hi there!\"]},\n","    {\"intent\": \"goodbye\", \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"], \"responses\": [\"Goodbye!\", \"See you later!\"]}\n","]\n","\n","# Data preprocessing\n","lemmatizer = WordNetLemmatizer()\n","\n","words = []\n","classes = []\n","documents = []\n","responses = {}\n","\n","for intent in data:\n","    for pattern in intent[\"patterns\"]:\n","        # Tokenize and lemmatize words\n","        tokens = word_tokenize(pattern)\n","        words.extend(tokens)\n","        documents.append((tokens, intent[\"intent\"]))\n","        classes.append(intent[\"intent\"])\n","\n","    responses[intent[\"intent\"]] = intent[\"responses\"]\n","\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in \"?\"]\n","words = sorted(list(set(words)))\n","\n","# Create training data\n","training = []\n","output_empty = [0] * len(classes)\n","\n","for document in documents:\n","    bag = []\n","    pattern_words = document[0]\n","    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n","\n","    for word in words:\n","        bag.append(1) if word in pattern_words else bag.append(0)\n","\n","    output_row = list(output_empty)\n","    output_row[classes.index(document[1])] = 1\n","    training.append([bag, output_row])\n","\n","X_train = np.array([i[0] for i in training])\n","y_train = np.array([i[1] for i in training])\n","\n","# Train a model (example using a simple neural network)\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","model = Sequential([\n","    Dense(128, input_shape=(len(X_train[0]),), activation='relu'),\n","    Dropout(0.5),\n","    Dense(64, activation='relu'),\n","    Dropout(0.5),\n","    Dense(len(y_train[0]), activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=1)\n","\n","# Save the model and preprocessing objects\n","model.save(\"chatbot_model.h5\")\n","pickle.dump({\"words\": words, \"classes\": classes, \"responses\": responses}, open(\"chatbot_data.pkl\", \"wb\"))\n"]}]}